# LLM-AIT
Adversarial and malicious testing tool/script for LLMS

# OpenAI Response Evaluation

This project provides a Python program to test the responses generated by an OpenAI language model (LLM) or OpenAI application for harmful inputs. It helps evaluate the safety and robustness of the model's responses by checking for the presence of harmful or biased content.

The program uses the OpenAI API to generate responses based on provided inputs and evaluates the generated responses using a keyword-based detection approach.

## Prerequisites

Before running the code, make sure you have the following:

- Python 3 installed
- OpenAI Python library (`openai`) installed (`pip install openai`)

You will also need an OpenAI API key to access the OpenAI API. Follow the OpenAI documentation to set up the API and obtain the key.

## Usage

1. Clone the repository or download the project files to your local machine.

2. Update the API key:
   - Open the Python script file (`llm_ait_cmd.py`) or (`llm_ait_csv.py`) as per your need.
   - Note: The file (`llm_ait_cmd.py`) will put the output on the terminal itself, while (`llm_ait_csv.py`) will dump it in a CSV.
   - Replace the placeholder value `"YOUR_API_KEY"` with your actual OpenAI API key.

3. Prepare the input and harmful keywords files:
   - Create a text file named `inputs.txt`.
   - Each line in the `inputs.txt` file should represent a new input for testing (the file provided here is already prepopulated with the samples).
   - Create a text file named `harmful_keywords.txt`.
   - Each line in the `harmful_keywords.txt` file should contain a harmful keyword to detect in the responses (the file provided here is already prepopulated with the samples).

4. Run the program:
   - Open a terminal or command prompt and navigate to the project directory.
   - Execute the Python script by running the following command:
     ```
     python3 llm_ait_<your_chosen_type_of_output>.py
     ```

5. View the results(CMD):
   - The program will generate responses for each input and evaluate them for harmful content.
   - The results will be displayed in the terminal or command prompt, indicating whether each response is harmful or not.

6. Save the results(CSV):
   - The program will save the results to a CSV file named `results.csv`.
   - The CSV file will contain columns for "Input", "Response", and "Harmful".
   - The file will be created in the same directory as the Python script.

## Customization

- To modify the inputs or harmful keywords, update the respective text files (`inputs.txt` and `harmful_keywords.txt`) as per your requirements.

- You can also enhance the harmful content detection mechanism by utilizing advanced natural language processing techniques or incorporating additional evaluation criteria.

## Notes

- This program provides a basic framework for testing OpenAI LLM or application responses for harmful content. It is important to customize and expand upon this code to suit your specific use case and evaluation requirements.

- Ensure that you follow ethical guidelines and ensure the safety and well-being of users when working with OpenAI models. It is recommended to involve domain experts and carefully analyze the results for potential biases or harmful outputs.

## License

This project is licensed under the [MIT License](LICENSE).
